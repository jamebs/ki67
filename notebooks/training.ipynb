{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 96\n",
    "CODENAME = f'f{SIZE}-s{SIZE}-m0'\n",
    "EXPERIMENT_NAME = f'densenet121-{CODENAME}'\n",
    "SHARD = 'amy'\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLIDES_PATH = Path('./data/results')\n",
    "EXPERIMENTS_PATH = Path('./data/experiments')\n",
    "TARGET = str(EXPERIMENTS_PATH / EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sources():\n",
    "    with open(EXPERIMENTS_PATH / 'config.json') as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    if SHARD not in data['shards']:\n",
    "        raise Exception('Invalid SHARD')\n",
    "\n",
    "    sources = [\n",
    "        str(SLIDES_PATH / slide / f'examples-{CODENAME}.tfrecords')\n",
    "        for name, shard in data['shards'].items()\n",
    "        for slide in shard\n",
    "        if name != SHARD\n",
    "    ]\n",
    "\n",
    "    size = int(len(sources) * TRAIN_TEST_SPLIT)\n",
    "    training = set(np.random.choice(sources, size=size, replace=False))\n",
    "    testing = set(sources).difference(training)   \n",
    "\n",
    "    validating = [\n",
    "        str(SLIDES_PATH / slide / f'examples-{CODENAME}.tfrecords')\n",
    "        for slide in data['shards'][SHARD]\n",
    "    ]\n",
    "\n",
    "    return training, testing, validating\n",
    "\n",
    "train_sources, test_sources, validating = get_sources()\n",
    "train_sources, test_sources, validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_steps, _ in enumerate(tf.data.TFRecordDataset(filenames=list(train_sources))):\n",
    "    pass\n",
    "print(f'Training records: {training_steps}')\n",
    "\n",
    "for testing_steps, _ in enumerate(tf.data.TFRecordDataset(filenames=list(test_sources))):\n",
    "    pass\n",
    "print(f'Testing records: {testing_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'data': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_function(proto):\n",
    "    example = tf.io.parse_single_example(proto, feature_description)\n",
    "    data = tf.io.decode_png(example['data'])\n",
    "    data = tf.cast(data, dtype=tf.float32) * (1.0 / 255.0)\n",
    "    data = tf.ensure_shape(data, (SIZE, SIZE, 3))\n",
    "    data = tf.clip_by_value(data, 0.0, 1.0)\n",
    "    label = tf.cast(example['label'], dtype=tf.bool)\n",
    "    label = tf.reshape(label, (1,))\n",
    "    return data, label\n",
    "\n",
    "def augment(inputs, target):\n",
    "    rgb = inputs\n",
    "    rgb = tf.image.random_flip_left_right(rgb)\n",
    "    rgb = tf.image.random_flip_up_down(rgb)\n",
    "    rgb = tf.image.random_brightness(rgb, 0.2)\n",
    "    rgb = tf.image.random_contrast(rgb, 0.7, 1.4)\n",
    "    rgb = tf.image.random_hue(rgb, 0.05)\n",
    "    rgb = tf.image.random_saturation(rgb, 0.7, 1.2)\n",
    "    rgb = tf.clip_by_value(rgb, 0.0, 1.0)\n",
    "    return rgb, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames=list(train_sources) + list(test_sources))\n",
    "dataset = dataset.map(_parse_function, tf.data.AUTOTUNE)\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset_augmented = dataset.map(augment, tf.data.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 8, figsize=(16, 10))\n",
    "for idx, (img, label) in dataset_augmented.take(32).enumerate():\n",
    "    i, j = idx % 8, (idx // 8) % 4\n",
    "    ax[j, i].imshow(img.numpy())\n",
    "    ax[j, i].set_title(label.numpy()[0])\n",
    "    ax[j, i].grid(None)\n",
    "    ax[j, i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 8, figsize=(16, 10))\n",
    "for idx, (img, label) in dataset.take(32).enumerate():\n",
    "    i, j = idx % 8, (idx // 8) % 4\n",
    "    ax[j, i].imshow(img.numpy())\n",
    "    ax[j, i].set_title(label.numpy()[0])\n",
    "    ax[j, i].grid(None)\n",
    "    ax[j, i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.data.TFRecordDataset(filenames=list(train_sources))\n",
    "training = training.map(_parse_function, tf.data.AUTOTUNE)\n",
    "training = training.shuffle(10000)\n",
    "training = training.repeat()\n",
    "training = training.map(augment, tf.data.AUTOTUNE)\n",
    "training = training.batch(BATCH_SIZE)\n",
    "training = training.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "testing = tf.data.TFRecordDataset(filenames=list(test_sources))\n",
    "testing = testing.map(_parse_function, tf.data.AUTOTUNE)\n",
    "testing = testing.repeat()\n",
    "testing = testing.batch(BATCH_SIZE)\n",
    "testing = testing.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "training, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ki67.modules.cnn.utils.model import DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model = DenseNet.create(shape=(SIZE, SIZE, 3))\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(LR),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(TARGET) / SHARD\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(str(model_dir / 'logs'), histogram_freq=1)\n",
    "logger = tf.keras.callbacks.CSVLogger(str(model_dir / 'training.csv'))\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(model_dir / 'weights.hdf5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training,\n",
    "    validation_data=testing,\n",
    "    steps_per_epoch=(training_steps // BATCH_SIZE),\n",
    "    validation_steps=(testing_steps // BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpointer, logger, tensorboard],\n",
    ")\n",
    "\n",
    "model.save(str(model_dir / 'training-model.hdf5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames=list(validating))\n",
    "dataset = dataset.map(_parse_function, tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet.create(shape=(SIZE, SIZE, 3))\n",
    "model.load_weights(str(Path(TARGET) / SHARD / 'weights.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(LR),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, precision, recall = model.evaluate(dataset, verbose=1)\n",
    "print(f'Accuracy: {(acc*100):5.2f}%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('conda': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}